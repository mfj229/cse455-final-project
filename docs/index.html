<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300&display=swap">
    <title>CSE455 Final Project</title>
  </head>
  <body>
    <header>
        <h1>CSE455 Final Project<br>Birds Birds Birds - Are they real?</h1>
        <p>Mika Feng</p>
    </header>

    <section>
      <h2>1. Problem</h2>
      <p>
        The problem that I tackled is the
        <a href=https://www.kaggle.com/competitions/birds23wi/overview>“Birds Birds Birds - Are they real?”</a>
        on Kaggle competition.
        I tried to achieve the highest accuracy attainable for this classification task.
      </p>
    </section>

    <section>
      <h2>2. Experiments and Results</h2>

      <section>
        <h3>2.1. Data Used</h3>
        <p>
          Data from Kaggle,
          <a href=https://www.kaggle.com/competitions/birds23wi/data>“Birds Birds Birds - Are they real?”</a>
        </p>
      </section>

      <section>
        <h3>2.2. Server Used</h3>
        <p>Google Colab Pro, GPU</p>
      </section>

      <section>
        <h3>2.3. Techniques Used</h3>

          <section>
            <h4>Code</h4>
            <p>
              The code of my final approach can be found
              <a href=https://github.com/mfj229/cse455-final-project>here</a>
              .
            </p>
          </section>

          <section>
            <h4>My final approach</h4>
            <p>
              I partitioned the initial training dataset into two subsets,
              namely the "training dataset" and the "validation dataset".
              The training dataset comprises 80% of the original training data,
              while the validation dataset comprises the remaining 20%.
            </p>
            <p>
              I performed fine-tuning on the EfficientNet V2 small model,
              and modified the model's classifier by adding a dropout layer with a dropout rate of 0.5.
              This helps to prevent overfitting and improve the model performance.
            </p>
            <p>
              For training, I used cross entropy loss function,
              AdamW optimizer with initial learning rate 0.0005 and weight decay 0.002,
              and a step learning rate scheduler with step size = 2 and gamma = 0.7.
            </p>
            <p>
              To augment the images during training,
              I utilized the RandomResizedCrop with scale=(0.70, 1.0)
              and RandomHorizontalFlip transformations.
            </p>
            <p>
              Using the above setting, after training 21 epochs,
              the training accuracy was 98.91%, the validation accuracy was 88.8%,
              the test accuracy (on Kaggle) was 88.3%, and each epoch took about 1000s to train.
            </p>
            <img src="accuracy_curve.png" alt="The accuracy of my model.">
          </section>
      </section>

      <section>
        <h3>2.4. Process and Issues</h3>

        <section>
          <h4>Linear Probing</h4>
          <p>
            At the beginning of this experiment, I tried linear probing,
            which trains a linear classifier on top of a pre-trained neural network
            on both Resnet50 and Resnet152 pretrained weights.
          </p>
          <p>
            For Resnet50, after training 10 epochs, the training accuracy was 73.2639%,
            the validation accuracy was 56.3595%, and each epoch took about 600s to train.
          </p>
          <p>
            For Resnet152, after training 10 epochs, the training accuracy was 80.8123%,
            the validation accuracy was 52.1619%, and each epoch took about 970s to train.
          </p>
          <p>
            The training accuracy increased
            but the validation accuracy didn't increase by increasing the layers of Resnet.
            Also, the time for training each epoch increased by about 370s.
            Therefore, the validation accuracy doesn't improve significantly enough
            with the use of linear probing on Resnet152.
          </p>
          <img src="Resnet50_vs_Resnet152.png" alt="The accuracy and the time per epoch of Resnet50 and Resnet152.">
        </section>

        <section>
          <h4>Fine-tuning</h4>
          <p>
            For Resnet50, I tried not freezing the model parameters.
            The result after training 10 epochs was training accuracy 95.5957%,
            validation accuracy 72.9863%.
            Therefore, fine-tuning all parameters results in significantly better validation accuracy
            compared to linear probing.
          </p>
          <img src="Linear_Probing_vs_Fine-tuning.png" alt="The accuracy of linear probing and fine-tuning.">
        </section>

        <section>
          <h4>Reduce overfitting</h4>
          <p>
            I experimented with image augmentation and adjusted the dropout and weight decay parameters.
            From my findings, adjusting the dropout parameter had the greatest impact.
            However, the improvement in validation accuracy was minimal despite this.
          </p>
        </section>

        <section>
          <h4>Improve validation accuracy</h4>
          <p>
            To improve the validation accuracy, I attempted to apply a weight sampler to the training set data loader,
            since there might be an issue with dataset imbalance.
            However, this approach did not lead to significant improvements in the validation accuracy.
          </p>
          <p>
            Next, I attempted to fine-tune the EfficientNet B0 model, which is more efficient in training,
            while also adjusting the dropout parameter in an effort to enhance the validation accuracy.
            After training 16 epochs, the training accuracy was 92.62%, the validation accuracy was 81.28%,
            the test accuracy (on Kaggle) was 80.6%, and each epoch took about 860s to train.
            Therefore, EfficientNet B0 performed better in accuracy and efficiency compared to Resnet50.
          </p>
          <p>
            In order to achieve higher accuracy, I tried two other different models vit_b_16 and EfficientNet V2.
          </p>
          <p>
            For vit_b_16, I used linear probing due to the high training cost.
            After training 6 epochs, the training accuracy was 97.40%, the validation accuracy was 87.15%,
            the test accuracy (on Kaggle) was 85.7%, and each epoch took more than 5000s to train.
          </p>
          <p>
            For EfficientNet V2, I fine tuned the model based on the small pretrained weight.
            After training 12 epochs, the training accuracy was 97.46%, the validation accuracy was 87.09%,
            the test accuracy (on Kaggle) was 86.8%, and each epoch took about 1000s to train.
          </p>
          <p>
            Since EfficientNet V2 performed better and took less time to train per epoch,
            I trained more epochs on it with AdamW optimizer.
            After training 21 epochs, the training accuracy was 98.91%,
            the validation accuracy was 88.8%, the test accuracy (on Kaggle) was 88.3%,
            and each epoch took about 1000s to train.
            The usage of AdamW instead of Adam can result in an enhancement in validation accuracy and a reduction in overfitting.
          </p>
          <img src="Training_performance_of_different_pre-trained_models.png" alt="Training performance of different pre-trained models.">
        </section>

      </section>

      <h2>3. Discussion</h2>
      <section>
        <h3>3.1. Findings from my experiment</h3>
        <p>
          Based on my experiments, I discovered two key findings:
        </p>
        <ul class = "margin-top">
          <li>Fine-tuning gives better results than linear probing.</li>
          <li>Utilizing a pre-trained model with higher accuracy can lead to improved validation accuracy.</li>
        </ul>
      </section>

      <section>
        <h3>3.2. Problem</h3>
        <p>There are two major problems:</p>
        <ul class = "margin-top">
          <li>
            The computing power of google colab pro GPU is not enough.
            For example, vit_b_16 is a good model but it costs too much computing power to train,
            and EfficientNet V2 M needs more GPU memory to train.
          </li>
          <li>
            Training data needs more image augmentation,
            since the number of images differs by the species of birds.
          </li>
        </ul>
      </section>

      <section>
        <h3>3.3. Next Steps</h3>
        <p>There are three things that I want to try:</p>
        <ul class = "margin-top">
          <li>
            Use a server with more memory and try the other pretrained models whose accuracy is beyond 88%,
            which is my highest test accuracy.
          </li>
          <li>
            Consider additional methods of image augmentation to further enhance accuracy.
          </li>
          <li>
            Try a pre-trained model trained with a larger image dataset, such as
            <a href="https://huggingface.co/google/vit-large-patch32-224-in21k">vit-large-patch32-224-in21k</a>
            .
          </li>
        </ul>
      </section>

      <section>
        <h3>3.4. How my approach differs from others</h3>
        <p>There are three primary ways in which my approach differs from others:</p>
        <ul class = "margin-top">
          <li>
            I added a dropout layer to the model's classifier to reduce overfitting.
          </li>
          <li>
            I perform fine-tuning on the pre-trained EfficientNet V2 model
            which is efficient and has a higher accuracy.
          </li>
          <li>
            I used AdamW instead of Adam as the optimizer.
            This resulted in an enhancement in validation accuracy and a reduction in overfitting.
          </li>
        </ul>
      </section>
    </section>

    <section>
      <h2>4. Previous Work</h2>
      <ul>
        <li><a href="https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/">
              Pytorch Image Classification using Transfer Learning</a></li>
        <li><a href="https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_v2_s.html">
              EFFICIENTNET_V2_S</a></li>
        <li><a href="https://qiita.com/omiita/items/1d96eae2b15e49235110">
              Will you be the strongest in 2021!? ? Explanation of the latest image recognition model EfficientNetV2</a></li>
        <li><a href="https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#eec7">
              A Visual Guide to Learning Rate Schedulers in PyTorch</a></li>
        <li><a href="https://towardsdatascience.com/improves-cnn-performance-by-applying-data-transformation-bf86b3f4cef4">
              Do and don’t when using transformation to improve CNN deep learning model</a></li>
        <li><a href="https://towardsdatascience.com/demystifying-pytorchs-weightedrandomsampler-by-example-a68aceccb452">
              Demystifying PyTorch’s WeightedRandomSampler by example</a></li>
      </ul>
    </section>

    <section>
      <h2>5. Project Presentation Video</h2>
      <iframe width="420" height="315" src="https://www.youtube.com/embed/i2ylREAMMKI">
</iframe>
    </section>
  </body>
</html>
